{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad0fd59-b2b0-4a80-8d63-40860f26e457",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca9ade-dfaf-47ab-9904-f279a64fbb37",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e45251-5a15-4204-9229-01afccb73620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Path handling\n",
    "from pathlib import Path\n",
    "\n",
    "# For General Data Wrangling and Calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "\n",
    "# For stop word filtering\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For tokenizing\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# For lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For TF-IDF transformation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For Word Embedding\n",
    "import sent2vec\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For UMAP\n",
    "from umap import UMAP\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072015f-08fd-4efd-ad41-3c35d1511222",
   "metadata": {},
   "source": [
    "Load input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a4db14-b774-4fb0-9b20-60d97daa83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (length: 11550):\n",
      "    condition_label                                   medical_abstract\n",
      "0                5  Tissue changes around loose prostheses. A cani...\n",
      "1                1  Neuropeptide Y and neuron-specific enolase lev...\n",
      "2                2  Sexually transmitted diseases of the colon, re...\n",
      "3                1  Lipolytic factors associated with murine and h...\n",
      "4                3  Does carotid restenosis predict an increased r...\n",
      "5                3  The shoulder in multiple epiphyseal dysplasia....\n",
      "6                2  The management of postoperative chylous ascite...\n",
      "7                4  Pharmacomechanical thrombolysis and angioplast...\n",
      "8                5  Color Doppler diagnosis of mechanical prosthet...\n",
      "9                5  Noninvasive diagnosis of right-sided extracard...\n",
      "Test set (length: 2888):\n",
      "    condition_label                                   medical_abstract\n",
      "0                3  Obstructive sleep apnea following topical orop...\n",
      "1                5  Neutrophil function and pyogenic infections in...\n",
      "2                5  A phase II study of combined methotrexate and ...\n",
      "3                1  Flow cytometric DNA analysis of parathyroid tu...\n",
      "4                4  Paraneoplastic vasculitic neuropathy: a treata...\n",
      "5                1  Treatment of childhood angiomatous diseases wi...\n",
      "6                1  Expression of major histocompatibility complex...\n",
      "7                1  Questionable role of CNS radioprophylaxis in t...\n",
      "8                5  Reversibility of hepatic fibrosis in experimen...\n",
      "9                2  Current status of duplex Doppler ultrasound in...\n",
      "Labels (length: 5):\n",
      "    condition_label                   condition_name\n",
      "0                1                        neoplasms\n",
      "1                2        digestive system diseases\n",
      "2                3          nervous system diseases\n",
      "3                4          cardiovascular diseases\n",
      "4                5  general pathological conditions\n"
     ]
    }
   ],
   "source": [
    "# Train data path\n",
    "train_df = Path(\"../data/Raw/medical_tc_train.csv\")\n",
    "# Test data path\n",
    "test_df = Path(\"../data/Raw/medical_tc_test.csv\")\n",
    "# Labels path\n",
    "labels_df = Path(\"../data/Raw/medical_tc_labels.csv\")\n",
    "\n",
    "# Open train data\n",
    "train_df = pd.read_csv(train_df)\n",
    "test_df = pd.read_csv(test_df)\n",
    "labels_df = pd.read_csv(labels_df)\n",
    "\n",
    "# Visualize top 10 rows\n",
    "print(f\"Training set (length: {len(train_df)}):\\n\", train_df.head(10))\n",
    "print(f\"Test set (length: {len(test_df)}):\\n\", test_df.head(10))\n",
    "print(f\"Labels (length: {len(labels_df)}):\\n\", labels_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e0948-aa32-45a2-b67b-8ce9ac6e8807",
   "metadata": {},
   "source": [
    "Add labels to train data for posterior visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b813d5a-5034-4596-a2a3-2451c9875695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_label</th>\n",
       "      <th>medical_abstract</th>\n",
       "      <th>condition_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Tissue changes around loose prostheses. A cani...</td>\n",
       "      <td>general pathological conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Neuropeptide Y and neuron-specific enolase lev...</td>\n",
       "      <td>neoplasms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sexually transmitted diseases of the colon, re...</td>\n",
       "      <td>digestive system diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Lipolytic factors associated with murine and h...</td>\n",
       "      <td>neoplasms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Does carotid restenosis predict an increased r...</td>\n",
       "      <td>nervous system diseases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition_label                                   medical_abstract  \\\n",
       "0                5  Tissue changes around loose prostheses. A cani...   \n",
       "1                1  Neuropeptide Y and neuron-specific enolase lev...   \n",
       "2                2  Sexually transmitted diseases of the colon, re...   \n",
       "3                1  Lipolytic factors associated with murine and h...   \n",
       "4                3  Does carotid restenosis predict an increased r...   \n",
       "\n",
       "                    condition_name  \n",
       "0  general pathological conditions  \n",
       "1                        neoplasms  \n",
       "2        digestive system diseases  \n",
       "3                        neoplasms  \n",
       "4          nervous system diseases  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.merge(train_df,labels_df, how = \"left\", on = \"condition_label\")\n",
    "test_df = pd.merge(test_df,labels_df, how = \"left\", on = \"condition_label\")\n",
    "\n",
    "# Visualize top rows\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8280e5b1-5546-4e82-8733-39147d515a55",
   "metadata": {},
   "source": [
    "## 1. Lower text and remove stopwords / punctuation\n",
    "Stop words are frequently found in all kind of texts since thay contribute to the sentence structure and meaning. However, they should be removed (depending on the analysis) to reduce the noise they might produce. Also, after removing them we will have a reduced text amount to process (data size reduction) and an improved performance. Besides, to have a more homogenous text it is necessary to lower all the text and remove punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811b8840-5d7e-45c5-acc7-3ab0973a0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c457e-0d8a-431d-b9ee-1e3f2ecefce0",
   "metadata": {},
   "source": [
    "Define function to remove stop words from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc3fc67-1e14-4b74-8241-eae53cfc3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_punctutation(data_df):\n",
    "\n",
    "    # Define tokenizer\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    \n",
    "    # Initialize loop to iterate through\n",
    "    for i, row in data_df.iterrows():\n",
    "    \n",
    "        # Get cell\n",
    "        text = row[\"medical_abstract\"]\n",
    "    \n",
    "        # Check if the cell contains text\n",
    "        if isinstance(text, str):\n",
    "        \n",
    "            # Lower and tokenize text\n",
    "            tokens = tokenizer.tokenize(text.lower())\n",
    "        \n",
    "            # Remove stop words\n",
    "            tokens_clean = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "            # Detokenize text\n",
    "            cleaned_sentence = \" \".join(tokens_clean)\n",
    "\n",
    "            # Save processed text\n",
    "            data_df.loc[i, \"medical_abstract\"] = cleaned_sentence\n",
    "\n",
    "    # Return cleaned text\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9d2a4-bd28-4b00-a9c5-0cce02abb5d2",
   "metadata": {},
   "source": [
    "Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f217798-d1f5-4918-8aa4-47d2ca3fff3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_abstract</th>\n",
       "      <th>condition_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obstructive sleep apnea following topical orop...</td>\n",
       "      <td>nervous system diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutrophil function pyogenic infections bone m...</td>\n",
       "      <td>general pathological conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phase ii study combined methotrexate teniposid...</td>\n",
       "      <td>general pathological conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flow cytometric dna analysis parathyroid tumor...</td>\n",
       "      <td>neoplasms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paraneoplastic vasculitic neuropathy treatable...</td>\n",
       "      <td>cardiovascular diseases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    medical_abstract  \\\n",
       "0  obstructive sleep apnea following topical orop...   \n",
       "1  neutrophil function pyogenic infections bone m...   \n",
       "2  phase ii study combined methotrexate teniposid...   \n",
       "3  flow cytometric dna analysis parathyroid tumor...   \n",
       "4  paraneoplastic vasculitic neuropathy treatable...   \n",
       "\n",
       "                    condition_name  \n",
       "0          nervous system diseases  \n",
       "1  general pathological conditions  \n",
       "2  general pathological conditions  \n",
       "3                        neoplasms  \n",
       "4          cardiovascular diseases  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "train_clean_df = remove_stopwords_punctutation(train_df)\n",
    "# Test data\n",
    "test_clean_df = remove_stopwords_punctutation(test_df.drop(columns = [\"condition_label\"]))\n",
    "# We drop the condition_label to not have it into account during the data exploration of data to be classified.\n",
    "\n",
    "# Visualize top rows\n",
    "#train_clean_df.head()\n",
    "test_clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e46b3f-8e60-4817-bc52-6e06645984b2",
   "metadata": {},
   "source": [
    "## 2. Lemmatize\n",
    "Lemmatizing reduce the words to their core meaning. For example, for words such as \"do\", \"doing\" and \"done\", lemmatizing will substitute them by \"do\", their core word. This process again reduces the amount of text to process (amount of total unique words), making the procesing more efficient.\n",
    "\n",
    "Define function to lemmatize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8816e56c-9d44-45c0-a4b0-1a4fa7f0bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_data(data_df):\n",
    "\n",
    "    # Define lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Define counter to count sentences that do not change with lemmatization\n",
    "    counter = 0\n",
    "    \n",
    "    # Initialize loop to iterate through\n",
    "    for i, row in data_df.iterrows():\n",
    "    \n",
    "        # Get cell\n",
    "        text = row[\"medical_abstract\"]\n",
    "    \n",
    "        # Check if the cell contains text\n",
    "        if isinstance(text, str):\n",
    "        \n",
    "            # Lower and tokenize text\n",
    "            tokens = word_tokenize(text)\n",
    "        \n",
    "            # Lemmatize\n",
    "            lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "            # Detokenize text\n",
    "            lemma_sentence = \" \".join(lemmas)\n",
    "\n",
    "            # Save processed text\n",
    "            data_df.loc[i, \"medical_abstract\"] = lemma_sentence\n",
    "\n",
    "            # Check if sentences are the same\n",
    "            if text == lemma_sentence:\n",
    "                counter += 1\n",
    "\n",
    "    # Report the amount of lemmatized sentences\n",
    "    if counter == len(data_df):\n",
    "        print(f\"All medical abstracts' text have remained the same after lemmatization: {counter}/{len(data_df)}\")\n",
    "\n",
    "        # Return cleaned text\n",
    "        return data_df\n",
    "    \n",
    "    elif counter < len(data_df):\n",
    "        print(f\"Some sentences have been lemmatized: {counter}/{len(data_df)}\")\n",
    "\n",
    "        # Return cleaned text\n",
    "        return data_df\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"Something went wrong: {counter}/{len(data_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4517f6a7-e5ca-4a16-a063-889d84010b2b",
   "metadata": {},
   "source": [
    "Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7010695a-36b0-4e43-92fa-c53fe1110f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentences have been lemmatized: 106/11550\n",
      "Some sentences have been lemmatized: 24/2888\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_abstract</th>\n",
       "      <th>condition_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obstructive sleep apnea following topical orop...</td>\n",
       "      <td>nervous system diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutrophil function pyogenic infection bone ma...</td>\n",
       "      <td>general pathological conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phase ii study combined methotrexate teniposid...</td>\n",
       "      <td>general pathological conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flow cytometric dna analysis parathyroid tumor...</td>\n",
       "      <td>neoplasms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paraneoplastic vasculitic neuropathy treatable...</td>\n",
       "      <td>cardiovascular diseases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    medical_abstract  \\\n",
       "0  obstructive sleep apnea following topical orop...   \n",
       "1  neutrophil function pyogenic infection bone ma...   \n",
       "2  phase ii study combined methotrexate teniposid...   \n",
       "3  flow cytometric dna analysis parathyroid tumor...   \n",
       "4  paraneoplastic vasculitic neuropathy treatable...   \n",
       "\n",
       "                    condition_name  \n",
       "0          nervous system diseases  \n",
       "1  general pathological conditions  \n",
       "2  general pathological conditions  \n",
       "3                        neoplasms  \n",
       "4          cardiovascular diseases  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "train_lemma_df = lemmatize_data(train_clean_df)\n",
    "# Test data\n",
    "test_lemma_df = lemmatize_data(test_clean_df)\n",
    "\n",
    "# Visualize top rows\n",
    "#train_lemma_df.head()\n",
    "test_lemma_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7910b3-b490-4031-8a51-e921e419be21",
   "metadata": {},
   "source": [
    "## 3. Text Vectorization\n",
    "Text vectorization is converting text into numeric data, by transforming the textual data into vectors.\n",
    "### 3.1. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "The TF-IDF technique measured the relevance of a term present in a document (medical abstract) within a larger corpus (the entire collection). To calculate the TF-IDF the following formula was applied:\n",
    "$$\n",
    "\\text{TF-IDF} = \\frac{n}{d} \\times \\log_{10} \\frac{N}{n}\n",
    "$$\n",
    "Where:\n",
    "- \\(n\\): The frequency of the term in the document.\n",
    "- \\(d\\): The total number of terms in the document.\n",
    "- \\(N\\): The total number of documents in the corpus.\n",
    "- \\(n_t\\): The number of documents that contain the term.\n",
    "- (TF): Term Frequency, calculated as:\n",
    "   $$\\frac{n}{d}$$\n",
    "- (IDF): Inverse Document Frequency, calculated as:\n",
    "  $$\\log_{10} \\frac{N}{n_t}$$\n",
    "\n",
    "Define function for performing the TF-IDF tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe6db5d-9697-42bc-bfba-f21412d31ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transformation(data_df):\n",
    "\n",
    "    # Define corpus\n",
    "    corpus = data_df[\"medical_abstract\"]\n",
    "\n",
    "    # Define and instance of the TF-ID vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform corpus into a matrix containing TF-IDF\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Return matrix\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add0bda-1151-42a7-924b-7258b0ecc2e9",
   "metadata": {},
   "source": [
    "Transform text to vector using TF-IDF tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f5bc47-7760-4eab-96eb-ab19e47023f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training lemmatized data\n",
    "train_raw_tdidf = tfidf_transformation(train_df)\n",
    "# Testing lemmatized data\n",
    "test_raw_tdidf = tfidf_transformation(test_df)\n",
    "\n",
    "# Training lemmatized data\n",
    "train_lem_tdidf = tfidf_transformation(train_lemma_df)\n",
    "# Testing lemmatized data\n",
    "test_lem_tdidf = tfidf_transformation(test_lemma_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be48bb-0b50-4bcd-adb8-a7114d27dee3",
   "metadata": {},
   "source": [
    "### 3.2. Sentence Embedding using BioSentVec\n",
    "Sentence embedding is a language modelling technique that converts sentences to vectors of real numbers. This technique represents a sentence in vector space with multiple dimensions. In this case, we will use a model called BioSentVec, which is a pre-trained embedding model for biomedical sentences. It was trained on PubMed and MIMIC-III texts.\n",
    "\n",
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a3a778-cbaf-4114-9f91-541c308f0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# Define the model location\n",
    "model_path = \"../model/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\n",
    "# Define the model instance\n",
    "model = sent2vec.Sent2vecModel()\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779990b3-a86e-46bd-b801-9fabd3be821c",
   "metadata": {},
   "source": [
    "Define function to transform sentence to vector using BioSentVec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5b08c6-ce6e-4962-916a-1e5af52629ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(data_df):\n",
    "    \n",
    "    # Initialize an empty list to store the sentence vectors\n",
    "    vectors = []\n",
    "    \n",
    "    # Initialize loop to iterate through\n",
    "    for i, row in data_df.iterrows():\n",
    "    \n",
    "        # Get cell\n",
    "        text = row[\"medical_abstract\"]\n",
    "    \n",
    "        # Check if the cell contains text\n",
    "        if isinstance(text, str):\n",
    "\n",
    "            # Retrieve vector from sentence\n",
    "            sentence_vector = model.embed_sentence(text)\n",
    "\n",
    "            # Append the vector to the list\n",
    "            vectors.append(sentence_vector[0])\n",
    "\n",
    "    # Return array with sentences' vectors\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d5df1-95a1-4698-bcf1-36950f107153",
   "metadata": {},
   "source": [
    "Transform sentences to vectors using BioSentVec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e935e523-7d60-4e83-a404-19acc62ca147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training lemmatized data\n",
    "train_raw_vec = sentence_to_vector(train_df)\n",
    "# Testing lemmatized data\n",
    "test_raw_vec = sentence_to_vector(test_df)\n",
    "\n",
    "# Training lemmatized data\n",
    "train_lem_vec = sentence_to_vector(train_lemma_df)\n",
    "# Testing lemmatized data\n",
    "test_lem_vec = sentence_to_vector(test_lemma_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c979c-e028-4401-9c13-e1e77e74640a",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n",
    "Since the data processing is the limitant step, specially the sent2vec transformation (it is necessary to load in memory the heavy sentence2vector model - more than 20Gb of RAM are needed), we will saved the processed data in pickle format.\n",
    "\n",
    "Save to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ee3d2a-b8fd-415c-b6e3-e1d1ef6f8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data wit labels\n",
    "train_df.to_pickle(Path(\"../data/Processed/train.pkl\")) # Training\n",
    "test_df.to_pickle(Path(\"../data/Processed/test.pkl\")) # Testing\n",
    "\n",
    "# TF-IDF Data\n",
    "np.save(Path(\"../data/Processed/train_raw_tfidf.npy\"), train_raw_tdidf)\n",
    "np.save(Path(\"../data/Processed/train_lemma_tfidf.npy\"), train_lem_tdidf)\n",
    "\n",
    "np.save(Path(\"../data/Processed/test_raw_tfidf.npy\"), test_raw_tdidf)\n",
    "np.save(Path(\"../data/Processed/test_lemma_tfidf.npy\"), test_lem_tdidf)\n",
    "\n",
    "# BioSent2Vec Data\n",
    "np.save(Path(\"../data/Processed/train_raw_s2v.npy\"), train_raw_vec)\n",
    "np.save(Path(\"../data/Processed/train_lemma_s2v.npy\"), train_lem_vec)\n",
    "\n",
    "np.save(Path(\"../data/Processed/test_raw_s2v.npy\"), test_raw_vec)\n",
    "np.save(Path(\"../data/Processed/test_lemma_s2v.npy\"), test_lem_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
